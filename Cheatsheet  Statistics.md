# üìåCheatsheet | Statistics DataScience


<details><summary><h2>Vocabulaire</h2></summary>

*   Stat **descriptives** (qui mesurent) vs probabilit√©s = statistiques **inf√©rentielles** (qui pr√©disent) : _descriptive sur le pass√©, inf√©rentielle sur le futur)_ 
    * descriptive => moyenne, √©cart-type, ...
    *   En stat inf√©rentielles, on utilise des tests satistiques = estimateurs = pour cr√©er des mod√®les statistiques
    

*   lignes = individus = unit√© d'observation = r√©alisation
    
*   colonnes = variables = caract√®res
    *   variables quantitatives : 
        * discr√®tes vs continues
        * timestamp = nb de secondes depuis 1jan1970 [cf unix time](www.epochconverter.com).  Format ISO 8601 = `1977-04-22T06:00:00Z`
    
    *   variables qualitatives => modalit√©s
        *   soit nominale (y/c bool√©en ) 
        - soit ordinale (grand, petit, etc)   
    * Noir = nominal (quali), ordinal (#quali), interval, ratio (all quanti)


* √©chantillon = jeu de donn√©es = dataset = observation
    - **echantillon <> population** 
    - distribution empirique =constat√©e.

* Midspread - Boxplot - boite √† moustache : 
    - Rappel $¬±1.œÉ = 68\%$ ; $¬±2.œÉ = 95\%$ ; $¬±3.œÉ = 99.7\% $
![image](https://user-images.githubusercontent.com/7408762/197854536-b36e92b2-3057-4bbe-a9d7-d12d7600148a.png)

* **Machine learning** = mod√©lisation statistique √† partir des donn√©es
    * deep learning = apprentissage direct √† partir des donn√©es brutes
    * les 5 classiques √† connaitre= **regression, Knn, SVM, r√©seaux de neurones, random forests**. 
        * supervis√© = j'ai un jeu d'apprentissage, sur lequel je connais le r√©sultat attendu = on esp√®re pr√©dire
        * non supervis√© = l'algorithme doit trouver tout seul les similarit√©s = on esp√®re d√©couvrir des relations latentes. Aussi ‚àÉ semi-supervised = une partie des donn√©es sont annot√©es
        * reinforcement learning = non supervis√©, mais r√©compense am√©liore les op√©rations (typique : jeu d'√©chec)
    * 3 sorties : quanti, quali, ordre (regression, classification, ranking)
    * Librairies python : scikit-learn, tensorflow, torch, theano, caffe. 
    * G√©n√©ralisation = pr√©dictions sur nouvelles donn√©es (test ou autre)


</details>

***

<h2> Using the tools : JupyterNB, Colab </h2>

<details><summary><h2> Colab </h2></summary>

### [dedicated cheatsheet on Colab](https://colab.research.google.com/drive/13IO3-gfyS9mSPuzAo6-wsYBUOVpxb_va?usp=sharing)
- **help** on all shortcuts => Ctrl  M  H
- create a new cell below/ above 'ctrl M B'  / `ctrl M A`
- convert a text cell to code / to text ctrl M  Y / ctrl M M
- run current cell 'ctrl + enter'
- delete a cell ctrl+m d

</details>

<details><summary><h2> Nettoyer </h2></summary>

7 types d'erreurs :
1.  **Valeurs manquantes**
2.  **Erreur lexicale** (e.g. texte quand nombre attendu, ou liste limitative de pays possibles,,)
3.  **Irr√©gularit√©** (e.g. cm quand m attendu)
    
4.  formatage incorrect
5. formatage parfois li√© √† hypoth√®ses de contenu ( e.g. 2 emails pour 1 personne))
6.  **doublon** (+ parfois **contradiction** si les doublons ont des valeurs diff√©rentes)
7.  valeur extr√®me = **atypique** (pas fausse) ou **aberrante** (fausse)
    
Comment r√©soudre les erreurs (Pr√©voir des aller-retours entre nettoyage et analyse) : 

<details> <summary> <h3> N.1. Imputer les valeurs manquantes </h3> </summary>

Bibliotheque sp√©cialis√©e : `missingno` 
1.  Trouver la bonne valeur (√† la main)
    
2.  Travailler avec un gruy√®re (donn√©es √† trou, selon le traitement statistique)
3.  Oublier la variable (si trop de trous)
    
4.  Amputer = oublier les individus (risque: les individus restants ne sont pas forc√©ment repr√©sentatifs)
    
5.  **Imputer** = deviner, e.g. imputation par la moyenne, ou imputer intelligemment, eg selon √¢ge pour la taille, ou m√©thode de hot-deck, Machine Learning / KNN, r√©gressions)

```python
myDF.isnull().sum() #somme par colonne le nb de manquant

data.loc[data['taille'].isnull(), 'taille'] = data['taille'].mean()
```

</details>

<details> <summary> <h3> N.2.Eliminer les doublons... si on peut </h3> </summary>

* Identifier les doublons : pas de r√®gles, √† identifier en fonction du contexte.

*   Regrouper en g√©rant les contradictions
    * methodes `myDF.duplicated() myDF.duplicate() myDF.unique()`
    * contradiction : √† ignorer, ou prendre la moyenne
    * parfois regroupement (information 1 individu r√©partie sur plusieurs lignes)

```python
data.loc[data.duplicated(keep=False),:] # duplicated returns Booleans


data['Dept'].value_counts()
# ou
data['Dept'].unique()
```

</details>

<details><summary><h3> N.3.Traiter les outliers (= valeur aberrantes)  </h3> </summary>

- Trouv√©es par Z-score ou √©cart interquartile IQR (outliers are defined as mild above Q3 + 1.5 IQR and extreme above Q3 + 3 IQR.)
    * midspread ou Z-score :  $z = (x ‚Äì Œº)/œÉ$ 
    * boite a moustache (boxplot)
- Trouver la bonne valeur (√†¬† la main)
- Supprimer la valeur ou conserver la valeur ... en fonction des √©tudes (e.g. moyenne vs m√©diane)
-  ... les valeurs **atypiques** sont int√©ressantes, et √† mentionner

![1024px-Boxplot_vs_PDF svg](https://user-images.githubusercontent.com/7408762/197854536-b36e92b2-3057-4bbe-a9d7-d12d7600148a.png)

</details>

<details><summary> <h3>N.4.Autres erreurs </h3></summary>

* On peut **supprimer** les individus avec erreur ... si ceux qui restent sont suffisants / non biais√©s.

* Erreur lexicale = souvent pas de correction possible
* Irr√©gularit√©, formatage  = parfois correction √† la main possible 

```python
data['nom_colonne'] = nouvelle_colonne
mask = # condition √† v√©rifier pour cibler sp√©cifiquement certaines lignes
data.loc[mask, 'ma_colonne'] = nouvelles_valeurs
VALID_COUNTRIES = ['France', 'C√¥te d\'ivoire', 'Madagascar', 'B√©nin', 'Allemagne'
, 'USA']
mask = ~data['pays'].isin(VALID_COUNTRIES)
data.loc[mask, 'pays'] = np.NaN

data['email'] = data['email'].str.split(',', n=1, expand=True)[0]

data['taille'] = data['taille'].str[:-1] # supprimer le dernier caractere
data['taille'] = pd.to_numeric(data['taille'], errors='coerce')
```

</details>    

</details>

* * *

<details><summary> <h2>Repr√©senter une variable par analyse monovari√©e</h2></summary>


<details><summary> <h3> R.1.Variables qualitatives :  </h3> </summary>


```python
# PIE CHART Diagramme en secteurs
data["categ"].value_counts(normalize=True).plot(kind='pie')
# Cette ligne assure que le pie chart est un cercle plut√¥t qu'une √©llipse
plt.axis('equal')
plt.show() # Affiche le graphique

# BAR CHART Diagramme en tuyaux d'orgues
data["categ"].value_counts(normalize=True).plot(kind='bar')
plt.show()

# TABLEAU avec effectifs, freq, freq cumul√©e
effectifs = myData["Modalit√©"].value_counts()
modalites = effectifs.index # l'index de effectifs (= de myData) contient les modalit√©s
myTable = pd.DataFrame(modalites, columns = ["Modalit√©"]) # cr√©ation du tableau √† partir des modalit√©s
myTable["effectif n"] = effectifs.values
myTable["frequency f"] = myTable["effectif n"]] / len(myData) # Rappel len(myDataFrame) renvoie la taille de l'√©chantillon = le nb de lignes
myTable = myTable.sort_values("Modalit√©") # tri des valeurs de la variable X (croissant)
myTable["Cumulated Freq F"] = myTable["frequency f"].cumsum() # cumsum calcule la somme cumul√©e
``` 

</details>

<details><summary> <h3> R.2.Variables quantitatives et moments  </h3> </summary>

```python
# BAR CHART pour var discretes # Diagramme en b√¢tons
data['quart_mois'] = [int((jour-1)*4/31)+1 for jour in data["date_operation"].dt.day]
data["quart_mois"].value_counts(normalize=True).plot(kind='bar',width=0.1)
plt.show()

# BAR CHART pour var continues = Histogramme
data["montant"].hist(density=True)
plt.show()
# Histogramme plus beau
data[data.montant.abs() < 100]["montant"].hist(density=True,bins=20)
plt.show()

#Fonction de r√©partition empirique (= histogramme cumul√©)

# Mesures de tendance centrale : 
data['montant'].mode() #renvoie un Series, car il peut y avoir plusieurs modes
data['montant'].mean()
data['montant'].median()

# Moments d'ordre 2
data['montant'].var() # variance empirique (avec biais)
data['montant'].var(ddof=0) #variance empirique (sans biais)
data['montant'].std()   # s ecart type 
data['montant'].std/data['montant'].mean() # coeff variation

data.boxplot(column="montant", vert=False) 
plt.show()

#Moments d'ordre 3 et 4
data['montant'].skew()
data['montant'].kurtosis()

```
**R√®gle de Sturges** = le nb de classes optimales est $(1+log2(n))$

**Variance empirique** = second moment = 
$v = \frac{1}{n} \sum_{i=1}^{n}(x_i-\overline{x})^2 = s^2$

**Variance empirique sans biais** = 
$s'^2 = \frac{1}{n-1} \sum_{i=1}^{n}(x_i-\overline{x})^2$
(pour un grand √©chantillon, pas de diff√©rence avec/sans biais)

**Coeff de variation** = $CV = \frac{s}{overline{x}}$ o√π $s$ l'√©cart type empirique ($\sigma$ √©cart type population) 

**Ecart Moyen Absolu** = variance avec norme 1 = $EMA = \frac{1}{n}\sum_{i=1}^{n}{|x_i - \overline{x}|}$ 

EMA peut aussi se calculer par √©cart √† la m√©diane. 

**Skewness =assym√©trie** = third standardised moment = $\tilde{\mu}_3 = \frac{\mu_3}{s^3} = \frac{1}{s^3}\frac{1}{n}\sum_{i=1}^{n}(x_i-\overline{x})^3$

Skweness > 0 = positive skew = long right tail = generally, mean>median

**Kurtosis =aplatissement** = fourth standardised moment = $\tilde{\mu}_4 = \frac{\mu_4}{s^4} = \frac{1}{s^4}\frac{1}{n}\sum_{i=1}^{n}(x_i-\overline{x})^4$

Kurtosis > 0 = positive curtosis = pointier than gaussian curve

(Note : first and second standardised moment are always 0 and 1)

</details>

<details><summary> <h3> R.3.Variables quantitatives et concentration  </h3> </summary>

Les 3 approches (avec l'exemple de la concentration des richesses): 
- Courbe de Lorenz = pauvres √† gauche, riches √† droite = escaliers de hauteur total 1=100%
    - la personne √† 50% de l'axe horizontal a le salaire m√©dian.
    - le salaire m√©dial est est celui de la personne correspondant √† la hauteur 50%.
- Indice de Bini = calcul√© sur la courbe de Lorenz 
    - Igini = 2 x aire entre Lorenz et la premi√®re bissectrice
    - Gini = 0 => √©galit√© parfaite
    - Gini = 1 => in√©galit√© parfaite (1 personne cumule tout)
- Pareto : "les X% les + riches poss√®dent Y% de la richesses"    

```python
depenses = data[data['montant'] < 0] dep = -depenses['montant'].values n = len(dep) lorenz = np.cumsum(np.sort(dep)) / dep.sum() lorenz = np.append([0],lorenz) # La courbe de Lorenz commence √† 0 
xaxis = np.linspace(0-1/n,1+1/n,n+1) #Il y a un segment de taille n pour chaque individu, plus 1 segment suppl√©mentaire d'ordonn√©e 0. Le premier segment commence √† 0-1/n, et le dernier termine √† 1+1/n. 
plt.plot(xaxis,lorenz,drawstyle='steps-post') 
plt.show()

AUC = (lorenz.sum() -lorenz[-1]/2 -lorenz[0]/2)/n # Surface sous la courbe de Lorenz. Le premier segment (lorenz[0]) est √† moiti√© en dessous de 0, on le coupe donc en 2, on fait de m√™me pour le dernier segment lorenz[-1] qui est √† moiti√© au dessus de 1. 
S = 0.5 - AUC # surface entre la premi√®re bissectrice et le courbe de Lorenz 
gini = 2*S gini
```

</details>

</details>

* * *

<details>
<summary> <h2> Repr√©senter des analyses bivari√©es </h2> </summary>

<details><summary><h3>B1. Corr√©lation et covariance</h3></summary>

**Scatterplot** diagramme de dispersion

**tableau de contingence** (`pivot_table`) contient les effectifs conjoints $n_{ij}$
- la distribution conjointe empirique sont $n_{ij}$
- la distribution marginale empirique de $X_i$ ou de $Y_j$ sont les sous-totaux $n_i$ ou $n_j$
- la distribution conditionnelle empirique de $X_i$ sachant $Y_0$ est une ligne/colonne d'effectifs conjoints

**Covariance** $cov(X,Y) = s_{X,Y} = \frac{1}{n}\sum_{i=1}^{n} (x_i-\overline{x}) (y_i‚àí\overline{y})$

**Correlation (lin√©aire) (de Pearson)** = covariance / les 2 √©carts-types
- entre -1 et 1
- n'est pas causalit√© (cf multiples exemples, dont le paradoxe de Simpson)
- ne d√©tecte que les relations lin√©aires (Cf exemple du cercle)

**Regression simple** $Y = a.X + b + \epsilon$ 
- o√π les estimateurs selon les Ordinary Least Squares (Moindres Carr√©s Ordinaires) sont : 
- $\hat{a} = \frac{cov(X,Y)}{s_X^2}$ et $\hat{b} = \overline{y} - \hat{a}.\overline{x}$
- $R^2$ = carr√© de la corr√©lation entre X et Y = % explicatif de la r√©gression (somme des carr√©s expliqu√©s / somme des carr√©s totaux)
- cette r√©gression lin√©aire est peu robustes aux valeurs aberrantes

```python
import scipy.stats as st # corr√©lation par scipy.stats.pearsonr
st.pearsonr(depenses["solde_avt_ope"],-depenses["montant"])[0]
# Par numpy = matrice de covariance, corr = valeur en [1,0]
np.cov(depenses["solde_avt_ope"],-depenses["montant"],ddof=0)[1,0]

import statsmodels.api as sm
Y = courses['montant']
X = courses[['attente']]
X = X.copy() # On modifiera X, on en cr√©e donc une copie
X['intercept'] = 1.
result = sm.OLS(Y, X).fit() # OLS = Ordinary Least Square (Moindres Carr√©s Ordinaire)
a,b = result.params['attente'],result.params['intercept']

plt.plot(courses.attente,courses.montant, "o")
plt.plot(np.arange(15),[a*x+b for x in np.arange(15)])
plt.xlabel("attente")
plt.ylabel("montant")
plt.show()
```

</details>

<details><summary> <h3>B.2. ANOVA = corr√©lation entre quanti et quali</h3></summary>

On d√©compose en 3 : 
- Total Sum of Squares = variation totale $= \sum\limits_{i=1}^{k} \sum\limits_{j=1}^{n_i} (y_{ij} ‚àí \overline{y})^2$
- Sum of Squares of the Model = variation interclasse (= somme des carr√©s expliqu√©s) $= \sum\limits_{i=1}^{k} n_i (\hat{y_{i}} ‚àí \overline{y})^2$
- Sum of Squares of the Error = variation intraclasse $= \sum\limits_{i=1}^{k}  \sum\limits_{j=1}^{n_i} (y_{ij} ‚àí \hat{y_i})^2 = \sum\limits_{i=1}^{k} n_i s_i^2$
- le rapport de corr√©lation est maintenant eta squared : 

$$Œ∑_{Y,X}^2 = \frac{V_{interclasses}}{V_{totale}}$$

```python
modalites = sous_echantillon[X].unique()
groupes = []

for m in modalites:
    groupes.append(sous_echantillon[sous_echantillon[X]==m][Y])
# Graphiques = points rouges pour la moyenne  
medianprops = {'color':"black"}
meanprops = {'marker':'o', 'markeredgecolor':'black',
            'markerfacecolor':'firebrick'} 
plt.boxplot(groupes, labels=modalites, showfliers=False, medianprops=medianprops, 
            vert=False, patch_artist=True, showmeans=True, meanprops=meanprops)
plt.show()
```

</details>


<details><summary><h3> B.3. Chi-2 entre 2 variables quali</h3></summary>


Le tableau de contingence compare les effectifs conjoints $n_{ij}$ aux effectifs pr√©vus en cas d'ind√©pendance $n_j . frequency(i) = n_j . n_i / n$

Cette comparaison donne une corr√©lation $\xi_{ij}$ qu'on represente avec une carte de chaleur _heatmap_. Valeur entre 0 et 1 = % de contribution √† la d√©pendance (non-ind√©pendance). 
La somme des contribution = 100%. 



```python
import seaborn as sns
tx = cont.loc[:,["Total"]]
ty = cont.loc[["Total"],:]
n = len(data)
indep = tx.dot(ty) / n
c = cont.fillna(0) # On remplace les valeurs nulles par 0
measure = (c-indep)**2/indep
xi_n = measure.sum().sum()
table = measure/xi_n
sns.heatmap(table.iloc[:-1,:-1],annot=c.iloc[:-1,:-1])
plt.show()
```

</details>

</details>

* * *

<details>
<summary> <h2> Composantes et clustering </h2> </summary>

Supervise => j'ai d√©ja des tag d'apprentissage. On parle de **classement**\= classification supervis√©e (en EN = "classification"). ¬†

Non supervis√© = **clustering**¬†

![](√∞≈∏‚Äú≈íCheatsheet  Statistics_files/Image.png)


* * *

Distance (erreur = risque = eloignement des donn√©es vs prediction modele)

Attention : erreur = risque empirique != performance du modele

*   erreur quadratique (le + utilis√©)
    

*   distance euclidienne = sqr(x^2 + y^2)
    

*   Distance manhattan = x + y
    
*   Pour chaines de caracteres = distance de Levenshtein = nbre mini d'operation (substitution, insertion, suppression) pour passer de l'une a l'autre.√Ç¬†
    

*   a connaitre = algo de Wagner et Fischer pour le calcul de la distance de Levenshtein.
    

algo param√©triques (eg regression = droite) => on cherche le parametre√Ç¬†√é¬∏ (qui peut etre multidimensionel)

algos non parametriques (+ complexit√©) => egg k-means qui est 'memory based' (garde toutes les donn√©es en memoire)

  

  

  

fuction loss = perte d'information

vraisemblance d'un jeu d'observations (x1...xN) par rapport √É¬† un mod√®le en statistiques est la fonction suivante :√Ç¬†√Ç¬†L(√é¬∏)=p(x1...xN|√é¬∏)√Ç¬†√Ç¬†.= proba d'avoir x1...xN sachant \\Theta

√Ç¬†√é¬∏^√Ç¬†avec un accent circonflexe√Ç¬†lorsqu'on parle d'un√Ç¬†estimateur (eet non de la valeur reelle, intrinseque)

  

* * *

<details>
<summary> <h3> C.1.  M√©thode factorielle = la + connue ACP </h3> </summary>

Factorielle :√Ç¬†

ACP√Ç¬† ( = EN PCA) = Principal component analysis

*   √Ç¬† √Ç¬† recehche d'un (hyperplan) avec moment d'inertie max (√©talement des points) = axe orthogonal √É¬† l'hyperplan = donne indication sur la variabilit√© =
    

*   espace Rp de dimension p variables, contient Ni le nuage des individus
    

*   Rechreche des corr√©lations entre variables√Ç¬†
    

*   espace Rn de dimension n individus, contient Np le nuage des variables
    

De pr√©f√©rence ACP norm√©e (centr√©e r√©duite)

3 graphiques :√Ç¬†

1.  1\. Pour l'objectif 1, ce sera la projection du nuage des individus NI sur les 2 premiers axes d√¢‚Ç¨‚Ñ¢inertie, c√¢‚Ç¨‚Ñ¢est-√É¬†-dire sur le premier plan factoriel.
    
2.  Le second s√¢‚Ç¨‚Ñ¢appelle le cercle des corr√©lations.
    
3.  2\. Pour l'objectif 2, ce sera la projection du nuage des variables NK sur le premier plan factoriel.
    

  

combien de composantes = min (p nbr de varialbes et n-1 nombre individus)

\=> eboulis des valeurs propres (class√©es en valeur d√©croissante)

\=> frequent de n'analyser que le 1er plan (2 composantes). Critere du coude - reperer le # o√É¬π le % inertie diminue + lentement. Criter de Kaiser (~contribution moyenen 100% / p)

</details>
  
<details>
<summary> <h3> C.2. Clulstering = Classification non supervis√©e = la + connue k-means (K-moyennes)
 </h3> </summary>

k-means√Ç¬†

k est un **hyperparam√®tre** (c'est √É¬† nous de l'optimiser, ce n'est pas l'algo qui va le proposer).√Ç¬†

  

  

Trainig set vs testing set = 80% / 20% des donn√©es fournies

  

  

* * *

Conversion de timestamp unix =√Ç¬†√Ç¬†[www.epochconverter.com](http://www.epochconverter.com/)√Ç¬†!

  

Erreur lexicale => Technique du dictionnaire.

Date => Format normalis√© ISO8601√Ç¬†1977-04-22T06:00:00Z.

</details>

</details>

* * *

<details>  
<summary> <h2> Pr√©parer ses donn√©es en sets <h2></summary>


**Centrer-reduire:**
- centrer r√©duire est n√©cessaire dans presque tous les cas. 
    - l'exception : la r√©gression simple 
- Autres transformations possibles ? 
```python 
from sklearn.preprocessing import StandardScaler
X = np.asarray(X) # Version numpy :
X = pd.DataFrame(X) # Version pandas :
X.describe().round( 2) # fonctionne slt avec la version pandas DF

# On instancie notre scaler :
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X) # on peut s√©parer le fit (calcul des mean et std) du transform (centrage et r√©duction)

pd.DataFrame(X_scaled).describe().round(2)
```

**Training sets:**: 
- Base : "Don't train on the testing set !" : le split tout simple : 
```python
# * Training split*(https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)
# NB Le 42 est un seed du random pour que ce soit toujours le m√™me split 
```
- ‚ÅâÔ∏è Oui mais si j'ai besoin de tester pour choisir un hyperparam√®tre? ‚áí d√©couper le training set en "folds" pour choisir un hyperparam√®tre = `GridSearch`
- ‚ÅâÔ∏è Oui mais si j'ai plusieurs mod√®les possibles ? ‚áí d√©couper le training set en "folds" pour voir comment chaque mod√®le se comporte "en moyenne" avec une **valildation crois√©e**
    * validation crois√©e = couper le jeu d'apprentissage en k parties (_k folds_) Chaque partie est utilis√©e comme jeu de test √† son tour. 
    * On peut faire LOO = Leave One Out = (k = N-1), mais pr√©f√©rable k= 5 ou 10 (pour temps de calcul)
    * stratifier la validation crois√©e = maintenir proportion de chaque classe 
    de la population dans les folds. 
    * PEut √™tre utiliser pour Grid-search ou Line-Search = pour tester diff√©rentes valeurs d'un hyperparam√®tre.

![image](https://scikit-learn.org/stable/_images/grid_search_workflow.png)



</details>

***

<details> <summary><h2>Pr√©dire </h2> </summary>

<details>  
<summary> <h3> M.1 Modeles predictifs lin√©aires = approximations supervis√©es </h3> </summary>

- Si lin√©arit√©+normalit√©+ind√©pendance (i.i.d.) => regression
    - recherche $Œ≤$ qui maximise la vraissemblance=  la probabilit√© de la distribution constat√©e ( $p(D|Œ≤)$ ) = minimise la somme des carr√©s des erreurs (MSE = RMSE)
        -  `LinearRegression` dans le module `linear_model`.
    
    <details> <summary>code</summary> 
            ```(python)
             ajouter ici code pandas
            ``` 

    </details>

    - $Œ≤=(X^‚ä§X)^{‚àí1}X^‚ä§y$ 
    - ... et si $X^TX$ non inversible (notamment si colonnes corr√©l√©es), utiliser pseudo-inversible. Mais le mod√®le (la signification des $Œ≤_i$) est alors moins interpr√©table...
    - si correlation, ou trop peu d'observation, la matrice des $X^TX$ n'est pas inversible => Sur-apprentissage car modele trop complexe
        - => Alors on minimise une fonction objectif = erreur + complexit√© 
        = minimum en $Œ≤$ du carr√© des erreurs + Œª.r√©gularisateur(Œ≤) = $min_{Œ≤ ‚àà \mathbb{R}^{p+1}} (y‚àíXŒ≤)^‚ä§(y‚àíXŒ≤) + Œª Regularisateur(Œ≤)$
        - o√π $Œª$ = hyperparam√®tre du poids de la regularisation (cf validation crois√©e)
        - **r√©gularisation de Tykhonov = ridge regression** pour diminuer le poids des coefs
            - regulateurs=carr√© de la norme de $Œ≤$ = norme $l2$
            - dans `scikit-learn : linear_model.Ridge` et `linear_model.RidgeCV` pour d√©terminer la valeur optimale du $Œª$ par validation crois√©e.
            - => toujours solution unique explicite $Œ≤=(ŒªI+X^‚ä§X)^{‚àí1}X^‚ä§y$
            - mais il faut **toujours standardiser** les variables $X$ pour $œÉ=1$ avec `sklearn.preprocessing.StandardScaler`
            - chemin de r√©gression : comment √©voluent les $Œ≤_j$ avec $Œª$, avec homog√©n√©isation des coeff pour les variables corr√©l√©es entre elles
[image](cheminregression.png)
        - **LASSO = modele parcimonieux (_sparse_)** pour r√©duire nombre de coeff $Œ≤$ = en avoir bcp nuls = 0
            - on utilise regularisateur norme1 de $Œ≤$
            - LASSO = _Least Absolute Shrinkage and Selection Operator_
            - si plusieurs variables corr√©l√©es, le Lasso va en choisir une seule au hazard => modele instable, solution non unique
            - Lasso est un **algo de r√©duction de dimension non supervis√©** 
        - **selection group√©e = elastic net** 
            - consiste √† combiner normes 1 et 2 sur $Œ≤$, avec cette fois 2 hyperparam√®tres $Œª$ et $Œ±$
            - $min_{Œ≤ ‚àà \mathbb{R}^{p+1}} (y‚àíXŒ≤)^‚ä§(y‚àíXŒ≤) + Œª ((1-Œ±)||Œ≤||_1 + Œ±)||Œ≤||_2)$
            - => solution moins parcimonieuse, mais plus stable que LASSO

![image](https://user.oc-static.com/upload/2019/06/10/15601584374984_LassoRidge.png)

- Evaluer la performance d'une r√©gression  
    - Avec ordre de grandeur : MSE et RMSE = mean squared error (mean of RSS = residual sum of squares = somme des carr√©s des r√©sidus)
    - Sans ordre de grandeur : RMSLE et R^2
        - RMSLE = squared log error, si on veut une comparer sur des donn√©es √† ordre de grandeur diff√©rents (erreur en % √©cart de la pr√©diction)
        - coef de d√©termination R^2 = 1- RSE (Relative Squared Error = erreur en % √©cart √† la moyenne) = corr√©lation de Pearson entre valeurs vraies et pr√©dites. See `sklearn.metrics.r2_score`


</details>
<details>
<summary> <h3> M.2 Mod√®les pr√©dictifs lin√©aires pour classification </h3> </summary>

- la matrice de confusion `metric.confusion_matrix(y_true, y_predict)` : 

| Confusion Matrix  |   True -      | True +          | 
|:----------------- |:-------------:|:---------------:|
| Predicted -       | True negative | False negative (type II)  |
| Predicted +       | False positive (type I) | True positive  |

- mesures √† connaitre: 
    - rappel (_recall_) = sensibilit√© (_sensitiviy_) = vrais positifs pr√©dits en %des vrais positifs 
    - precision = vrais positifs pr√©dits en %des positifs pr√©dits
    - F-score = moyenne harmonique rappel & precision
    - sp√©cificit√© = vrais n√©gatifs pr√©dits en % des vrais n√©gatifs = sensibilit√© sur les n√©gatifs
    - AUROC = area under ROC curve (1 pour classifieur parfait, 0.5 pour classifieur al√©atoire)

ici mettre image courbe ROC sensibilit√© vs 1-spe√ßificit√©


- regression logistique = pour classification binaire
    - classification binaire =  $y$ vaut 0 ou 1.
    - on on ne pr√©dit plus les valeurs, mais la probabilit√© $p(y = 1|x)$ compos√©e avec la fonction logistique $u\mapsto {1\over{1+e^{-u}}} $
    - Pas de solution exacte, calcul num√©rique par gradient
    - Pour √©viter le sur-apprentissage, r√©gularisation  ‚Ñì2 (par d√©faut dans `scikit-learn`, 
    - Pour un mod√®le parcimonieux, r√©gularisation ‚Ñì1 (dans `scikit-learn`, option`'penalty'=l1`
- SVM binaire = support vector machine = separatrice a vaste marge
    - recherche d'un hyperplan s√©parateur maximisant la marge
    - risque d'erreur (observations impossibles √† s√©parer par hyperplan, typiquement outliers). On utilise Hinge loss = perte charniere
    - 
- SVM multiclasse : regression multiple classes
    - one-versus-rest OVR = One-versus-all = OVA
        - on construit k SVM, en cherchant √† optimiser
    - one-versus-one OVO
- evaluer la qualit√© d'une pr√©diction
        - [`sklearn.metrics.mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html') pour calculer MSE ou RMSE entre la pr√©diction et la r√©alit√©. (R= root square)\
</details>

<details>
<summary> <h3> M.3 Mod√®les pr√©dictifs non lin√©aires </h3> </summary>
- "Kernel trick" : transformer les x d'input des 
![image](https://user-images.githubusercontent.com/7408762/197527731-29e2ad2b-2a1e-48a7-b92c-26df55445280.png)

- Neural networks : fonction d'activation sur entr√©es. "Perceptron"
    - Le Perceptron = "neurone" : 
        - Combi lin√©aire des entr√©es x activation
        - poids appris par descende de gradient
    - Empiler les perceptrons : 
        - poids sur chaque perceptron
        - √† entrainer avec EN back-propagation (FR r√©tro-propagation) : $derreur/dw_hji= d/d * d/d * d/d$

    - Pour approximation
        - technique de descente du gradient
        - entropie crois√©e 
    - Pour classification
        - possible d'utiliser activation √† seuil
        - mieux : utiliser sigmoide (typiquement : activation logistique) pour probabilit√© d'appartenance √† une classe 
    - Limitation : les r√©seaux de neurones ne sont pas la solution √† tous les probl√®mes car...
        
</details>


<details>
<summary> <h3> M.4 Mod√®les ensemblistes </h3> </summary>

- Gist = combine several models together
    - "Bootstrap" first idea = sampling with remise √©chantillonage avec remise
    - M√©thodes parall√®les: train several models simultaneously, recombine them at the end
    - utilisant des "apprenants faibles" : des m√©thodes simples et peu efficaces, qui en se combinant donnent de meilleurs r√©sultat que les m√©thodes complexes
    - M√©thodes s√©quentielles : **boosting**

<details>
<summary> **Bagging** = Bootstrap aggregation </summary>
    - Moyenne pour pr√©diction, vote majoritaire pour classification
    - 
```(python)
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=100, noise=0.25)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

from sklearn.ensemble import BaggingClassifier 

bagging = BaggingClassifier(n_estimators=5)
bagging.fit(X_train, y_train)
from mglearn.plot_interactive_tree import plot_tree_partition
from mglearn.plot_2d_separator import plot_2d_separator
from mglearn.tools import discrete_scatter

fig, axes = plt.subplots(2, 3, figsize=(20, 10))
for i, (ax, tree) in enumerate(zip(axes.ravel(), bagging.estimators_)):
    ax.set_title("Tree {}".format(i))
    plot_tree_partition(X_train, y_train, tree, ax=ax)
plot_2d_separator(bagging, X_train, fill=True, ax=axes[-1, -1],
                                    alpha=.4)
axes[-1, -1].set_title("Bagging")
discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)
``` 

</details>

<details>
<summary>
- **Random Forest** = arbres de d√©cisions binaires combin√©s  la majorit√© de vote
</summary>
    - Pb: les arbres de d√©cision ont tendance √† overfitter. 
    - Pour faire grandir chaque noeud, on n'utilise qu'un sous-ensemble de features (et pas toutes comme le bagging).
        - sous ensemble choisi de mani√®re al√©atoire : arbres al√©atoires
    - Avantage  : complexit√© peu √©lev√©s, on a estimation de l'importance des features. Pas d'overfitting, peu de m√©moire utilis√©e. 
 ```(python)   
import pandas as pd

train = pd.read_csv("train.csv")
test  = pd.read_csv("test.csv")
print(train.shape)
train.isna().sum()
train = train.loc[train.Activity.notna()]
train = train.fillna(train.median(), inplace=True)

X_train = train[train.columns[:-2]]
y_train = train['Activity']

X_test = test[test.columns[:-2]]
y_test = test['Activity']

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=500, oob_score=True)
model = rfc.fit(X_train, y_train)
from sklearn.metrics import accuracy_score

pred = rfc.predict(X_test)
print("accuracy {:.2f}".format(accuracy_score(y_test, pred)))
from sklearn.feature_selection import SelectFromModel
select = SelectFromModel(rfc, prefit=True, threshold=0.003)
X_train2 = select.transform(X_train)
print(X_train2.shape)
import timeit

rfc2 = RandomForestClassifier(n_estimators=500, oob_score=True)

start_time = timeit.default_timer()

rfc2 = rfc2.fit(X_train2, y_train)

X_test2 = select.transform(X_test)

pred = rfc2.predict(X_test2)
elapsed = timeit.default_timer() - start_time
accuracy = accuracy_score(y_test, pred)

print("accuracy {:.2f} time {:.2f}s".format(accuracy, elapsed))

```

</details>

<details>
<summary> Boosting & Gradient Boosting </summary>

- Le Boosting, dont adaboost
    - on pondere chacun des points √† chaque generation

- Gradient de Boosting : 
    - jhkjhj

</details>

</details>

<details>
<summary> <h3> P.5 Mesurer la performance de la pr√©vision </h3> </summary>

* Il y a de multiples scores de performance (si 'score", alors + haut = mieux, si "error", l'inverse), [liste ici](https://scikit-learn.org/stable/modules/model_evaluation.html), les classiques: 
    * Pour les r√©gressions : mean_squared_error, mean_absolute_percentage_error, r2_score
    * Pour les classifications : accuracy, precision (no false positive), recall (no missing positive), F1 (une moyenne entre precision et recall)
    * Pour les regroupements (clustering): completeness, homogeinity, mutual information


Le compromis biais-variance : 
![image](https://user-images.githubusercontent.com/7408762/200091906-6977561e-4cdf-4097-b45a-7775aebf0a5e.png)

* biais d'induction = inductive bias = hypoth√®se √† ajouter pour arriver √† un "bon" mod√®le. Typique des "ill-posed problems" (probl√®mes mal pos√©s). 
 
* Pour une pr√©vision quanti : on calcule un MSE / RMSE ou un `r2_score` 
* Pour une classification : `accuracy` =  % des classes correctes


</details>

</details>

<details>
<summary> Sources </summary>
---
[Cheatsheet Anthony : https://asardell.github.io/statistique-python/](https://asardell.github.io/statistique-python/)

[Meme contenu copi√© sur evernote](evernote:///view/6367254/s57/f1dae14f-b0c0-4024-a6f5-7b2535f53308/67117fc9-036c-4028-b61e-04a2b3349d73/)
</details
>
---

***


chgt 02nov22 2021

*** 
