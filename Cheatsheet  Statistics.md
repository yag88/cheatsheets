# üìåCheatsheet | Statistics


<details> <summary> <h2>Vocabulaire </h2></summary>

*   Stat **descriptives** (qui mesurent) vs probabilit√©s = statistiques **inf√©rentielles** (qui pr√©disent) : _descriptive sur le pass√©, inf√©rentielle sur le futur)_ 
    * descriptive => moyenne, √©cart-type, ...
    *   En stat inf√©rentielles, on utilise des tests satistiques = estimateurs = pour cr√©er des mod√®les statistiques
    

*   lignes = individus = unit√© d'observation = r√©alisation
    
*   colonnes = variables = caract√®res
    *   variables quantitatives : 
        * discr√®tes vs continues
        * timestamp = nb de secondes depuis 1jan1970 [cf unix time](www.epochconverter.com).  Format ISO 8601 = `1977-04-22T06:00:00Z`
    
    *   variables qualitatives => modalit√©s
        *   soit nominale (y/c bool√©en ) 
        - soit ordinale (grand, petit, etc)   
    * Noir = nominal (quali), ordinal (#quali), interval, ratio (all quanti)


* √©chantillon = jeu de donn√©es = dataset = observation
    - **echantillon <> population** 

* Midspread - Boxplot - boite √† moustache : 
![image]https://user-images.githubusercontent.com/7408762/197854536-b36e92b2-3057-4bbe-a9d7-d12d7600148a.png    

</details>

<details> <summary> <h2> Nettoyer </h2></summary>

7 types d'erreurs :
1.  **Valeurs manquantes**
2.  **Erreur lexicale** (e.g. texte quand nombre attendu, ou liste limitative de pays possibles,,)
3.  **Irr√©gularit√©** (e.g. cm quand m attendu)
    
4.  formatage incorrect
5. formatage parfois li√© √† hypoth√®ses de contenu ( e.g. 2 emails pour 1 personne))
6.  **doublon** (+ parfois **contradiction** si les doublons ont des valeurs diff√©rentes)
7.  valeur extr√®me = **atypique** (pas fausse) ou **aberrante** (fausse)
    
Comment r√©soudre les erreurs (Pr√©voir des aller-retours entre nettoyage et analyse) : 

<details> <summary> <h3> N.1. Valeurs manquantes : imputation </h3> </summary>

Bibliotheque sp√©cialis√©e : `missingno` 
1.  Trouver la bonne valeur (√† la main)
    
2.  Travailler avec un gruy√®re (donn√©es √† trou, selon le traitement statistique)
3.  Oublier la variable
    
4.  Oublier les individus (mais les individus restants ne sont pas forc√É¬©ment repr√©sentatifs)
    
5.  **Imputer** = deviner, e.g. imputation par la moyenne, ou imputer intelligemment, eg selon √¢ge pour la taille, ou m√©thode de hot-deck, Machine Learning / KNN, r√©gressions)

```(python)
myDF.isnull().sum() #somme par colonne le nb de manquant
data['nom_colonne'] = nouvelle_colonne
mask = # condition √† v√©rifier pour cibler sp√©cifiquement certaines lignes
data.loc[mask, 'ma_colonne'] = nouvelles_valeurs

data['taille'] = data['taille'].str[:-1] # supprimer le dernier caractere
data['taille'] = pd.to_numeric(data['taille'], errors='coerce')

data['Dept'].value_counts()
# ou
data['Dept'].unique()
```

</details>

<details> <summary> <h3> N.2.Eliminer les doublons... si on peut </h3> </summary>

* Identifier les doublons : pas de r√®gles, √† identifier en fonction du contexte.

*   Regrouper en g√©rant les contradictions
    * methodes `myDF.duplicated() myDF.duplicate() myDF.unique()`
    * contradiction : √† ignorer, ou prendre la moyenne
    * parfois regroupement (information 1 individu r√©partie sur plusieurs lignes)

```(python)
data.loc[data.duplicated(keep=False),:]
```

</details>

<details> <summary> <h3> N.3.Traiter les outliers (= valeur aberrantes)  </h3> </summary>

-    trouv√©es par Z-score ou √©cart interquartile IQR (outliers are defined as mild above Q3 + 1.5 IQR and extreme above Q3 + 3 IQR.)
    * midspread ou Z-score, 
    * boite a moustache (boxplot)
-  Trouver la bonne valeur (√†¬† la main)
-  Supprimer la valeur ou conserver la valeur ... en fonction des √©tudes (e.g. moyenne vs m√©diane)
-  ... les valeurs **atypiques** sont int√©ressantes, et √† mentionner

![1024px-Boxplot_vs_PDF svg](https://user-images.githubusercontent.com/7408762/197854536-b36e92b2-3057-4bbe-a9d7-d12d7600148a.png)

</details>

<details><summary> <h3>N.4.Autres erreurs </h3></summary>

* On peut **supprimer** les individus avec erreur ... si ceux qui restent sont suffisants / non biais√©s.

* Erreur lexicale = souvent pas de correction possible
* Irr√©gularit√©, formatage  = parfois correction √† la main possible 

</details>    

* * *

<summary> <h2> Representer des variables </h2> </summary>

  

  

  

  

  

  

  
</details>
* * *

<details>
<summary> <h2> Composantes et clustering </h2> </summary>

Supervise => j'ai d√©ja des tag d'apprentissage. On parle de **classement**\= classification supervis√É¬©e (en EN = "classification").√Ç¬†

Non supervis√É¬© =√Ç¬† **clustering√Ç**¬†

![](√∞≈∏‚Äú≈íCheatsheet  Statistics_files/Image.png)

  

D

* * *

Distance (erreur = risque = eloignement des donn√É¬©es vs prediction modele)

Attention : erreur = risque empirique != performance du modele

*   erreur quadratique (le + utilis√É¬©)
    

*   distance euclidienne = sqr(x^2 + y^2)
    

*   Distance manhattan = x + y
    
*   Pour chaines de caracteres = distance de Levenshtein = nbre mini d'operation (substitution, insertion, suppression) pour passer de l'une a l'autre.√Ç¬†
    

*   a connaitre = algo de Wagner et Fischer pour le calcul de la distance de Levenshtein.
    

algo param√É¬©triques (eg regression = droite) => on cherche le parametre√Ç¬†√é¬∏ (qui peut etre multidimensionel)

algos non parametriques (+ complexit√É¬©) => egg k-means qui est 'memory based' (garde toutes les donn√É¬©es en memoire)

  

  

  

fuction loss = perte d'information

vraisemblance d'un jeu d'observations (x1...xN) par rapport √É¬† un mod√É¬®le en statistiques est la fonction suivante :√Ç¬†√Ç¬†L(√é¬∏)=p(x1...xN|√é¬∏)√Ç¬†√Ç¬†.= proba d'avoir x1...xN sachant \\Theta

√Ç¬†√é¬∏^√Ç¬†avec un accent circonflexe√Ç¬†lorsqu'on parle d'un√Ç¬†estimateur (eet non de la valeur reelle, intrinseque)

  

* * *

1.  M√É¬©thode factorielle = la + connue ACP
    
2.  Clulstering = Classification non supervis√É¬©e = la + connue k-means (K-moyennes)
    

  

Factorielle :√Ç¬†

ACP√Ç¬† ( = EN PCA) = Principal component analysis

*   √Ç¬† √Ç¬† recehche d'un (hyperplan) avec moment d'inertie max (√É¬©talement des points) = axe orthogonal √É¬† l'hyperplan = donne indication sur la variabilit√É¬© =
    

*   espace Rp de dimension p variables, contient Ni le nuage des individus
    

*   Rechreche des corr√É¬©lations entre variables√Ç¬†
    

*   espace Rn de dimension n individus, contient Np le nuage des variables
    

De pr√É¬©f√É¬©rence ACP norm√É¬©e (centr√É¬©e r√É¬©duite)

3 graphiques :√Ç¬†

1.  1\. Pour l'objectif 1, ce sera la projection du nuage des individus NI sur les 2 premiers axes d√¢‚Ç¨‚Ñ¢inertie, c√¢‚Ç¨‚Ñ¢est-√É¬†-dire sur le premier plan factoriel.
    
2.  Le second s√¢‚Ç¨‚Ñ¢appelle le cercle des corr√É¬©lations.
    
3.  2\. Pour l'objectif 2, ce sera la projection du nuage des variables NK sur le premier plan factoriel.
    

  

combien de composantes = min (p nbr de varialbes et n-1 nombre individus)

\=> eboulis des valeurs propres (class√É¬©es en valeur d√É¬©croissante)

\=> frequent de n'analyser que le 1er plan (2 composantes). Critere du coude - reperer le # o√É¬π le % inertie diminue + lentement. Criter de Kaiser (~contribution moyenen 100% / p)

  

k-means√Ç¬†

k est un **hyperparam√É¬®tre** (c'est √É¬† nous de l'optimiser, ce n'est pas l'algo qui va le proposer).√Ç¬†

  

  

Trainig set vs testing set = 80% / 20% des donn√É¬©es fournies

  

  

* * *

Conversion de timestamp unix =√Ç¬†√Ç¬†[www.epochconverter.com](http://www.epochconverter.com/)√Ç¬†!

  

Erreur lexicale => Technique du dictionnaire.

Date => Format normalis√É¬© ISO8601√Ç¬†1977-04-22T06:00:00Z.

  

  

</details> 

* * *

<details>  
<summary>
** Centrer-reduire, training split :** </summary>

```import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
```
D√É¬©finissons nos donn√É¬©es :

  

\# Notre matrice de base :

X = \[\[12,√Ç¬† √Ç¬† 30,√Ç¬† √Ç¬† 80,√Ç¬† -100\],√Ç¬† √Ç¬† \[-1000, 12,√Ç¬† √Ç¬† -23,√Ç¬† 10\],√Ç¬† √Ç¬† \[14,√Ç¬† √Ç¬† 1000,√Ç¬† 0,√Ç¬† √Ç¬† 0\]\]

  

\# Version numpy :

X = np.asarray(X)

\# Version pandas :

X = pd.DataFrame(X)

Avec √Ç¬†pandas√Ç¬† , on peut calculer la moyenne et l'√É¬©cart-type de chaque dimension√Ç¬†:

  

\# On applique la methode .describe() pour avoir la moyenne et la .std(), et la m√É¬©thode .round(2) pour arrondir √É¬† 2 d√É¬©cimales apr√É¬®s la virgule :

X.describe()

On peut ensuite √Ç¬´√Ç¬†scaler√Ç¬†√Ç¬ª nos donn√É¬©es :

  

\# On instancie notre scaler :

scaler = StandardScaler()

\# On le fit :

scaler.fit(X)

\# On l'entraine :

X\_scaled = scaler.transform(X)

\# On peut faire les 2 op√É¬©rations en une ligne :

X\_scaled = scaler.fit\_transform(X)

\# On le transforme en DataFrame :

X\_scaled = pd.DataFrame(X\_scaled)

\# On peut appliquer la m√É¬©thode .describe() et .round()

X\_scaled.describe().round(2)

* Training split*

(https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
`X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.33, random_state=42)`
Le 42 est un seed du random pour que ce soit toujours le m√™me 

</details>

***

<details>  


<summary>
<h2>Modeles pr√©dictifs:</h2> </summary>

<details>  
<summary>
<h3> M.1 Modeles predictifs lin√©aires = approximations supervis√©es </h3>
</summary>

- Si lin√©arit√©+normalit√©+ind√©pendance (i.i.d.) => regression
    - recherche $Œ≤$ qui maximise la vraissemblance=  la probabilit√© de la distribution constat√©e ( $p(D|Œ≤)$ ) = minimise la somme des carr√©s des erreurs (MSE = RMSE)
        -  `LinearRegression` dans le module `linear_model`.
    
    <details> <summary>code</summary> 
            ```(python)
             ajouter ici code pandas
            ``` 

    </details>

    - $Œ≤=(X^‚ä§X)^{‚àí1}X^‚ä§y$ 
    - ... et si $X^TX$ non inversible (notamment si colonnes corr√©l√©es), utiliser pseudo-inversible. Mais le mod√®le (la signification des $Œ≤_i$) est alors moins interpr√©table...
    - si correlation, ou trop peu d'observation, la matrice des $X^TX$ n'est pas inversible => Sur-apprentissage car modele trop complexe
        - => Alors on minimise une fonction objectif = erreur + complexit√© 
        = minimum en $Œ≤$ du carr√© des erreurs + Œª.r√©gularisateur(Œ≤) = $min_{Œ≤ ‚àà \mathbb{R}^{p+1}} (y‚àíXŒ≤)^‚ä§(y‚àíXŒ≤) + Œª Regularisateur(Œ≤)$
        - o√π $Œª$ = hyperparam√®tre du poids de la regularisation (cf validation crois√©e)
        - **r√©gularisation de Tykhonov = ridge regression** pour diminuer le poids des coefs
            - regulateurs=carr√© de la norme de $Œ≤$ = norme $l2$
            - dans `scikit-learn : linear_model.Ridge` et `linear_model.RidgeCV` pour d√©terminer la valeur optimale du $Œª$ par validation crois√©e.
            - => toujours solution unique explicite $Œ≤=(ŒªI+X^‚ä§X)^{‚àí1}X^‚ä§y$
            - mais il faut **toujours standardiser** les variables $X$ pour $œÉ=1$ avec `sklearn.preprocessing.StandardScaler`
            - chemin de r√©gression : comment √©voluent les $Œ≤_j$ avec $Œª$, avec homog√©n√©isation des coeff pour les variables corr√©l√©es entre elles
[image](cheminregression.png)
        - **LASSO = modele parcimonieux (_sparse_)** pour r√©duire nombre de coeff $Œ≤$ = en avoir bcp nuls = 0
            - on utilise regularisateur norme1 de $Œ≤$
            - LASSO = _Least Absolute Shrinkage and Selection Operator_
            - si plusieurs variables corr√©l√©es, le Lasso va en choisir une seule au hazard => modele instable, solution non unique
            - Lasso est un **algo de r√©duction de dimension non supervis√©** 
        - **selection group√©e = elastic net** 
            - consiste √† combiner normes 1 et 2 sur $Œ≤$, avec cette fois 2 hyperparam√®tres $Œª$ et $Œ±$
            - $min_{Œ≤ ‚àà \mathbb{R}^{p+1}} (y‚àíXŒ≤)^‚ä§(y‚àíXŒ≤) + Œª ((1-Œ±)||Œ≤||_1 + Œ±)||Œ≤||_2)$
            - => solution moins parcimonieuse, mais plus stable que LASSO

- Evaluer la performance d'une r√©gression  
    - Avec ordre de grandeur : MSE et RMSE = mean squared error (mean of RSS = residual sum of squares = somme des carr√©s des r√©sidus)
    - Sans ordre de grandeur : RMSLE et R^2
        - RMSLE = squared log error, si on veut une comparer sur des donn√©es √† ordre de grandeur diff√©rents (erreur en % √©cart de la pr√©diction)
        - coef de d√©termination R^2 = 1- RSE (Relative Squared Error = erreur en % √©cart √† la moyenne) = corr√©lation de Pearson entre valeurs vraies et pr√©dites. See `sklearn.metrics.r2_score`


</details>
<details>
<summary> <h3> M.2 Mod√®les pr√©dictifs lin√©aires pour classification </h3> </summary>

- regression logistique = pour classification binaire
    - classification binaire =  $y$ vaut 0 ou 1.
    - on on ne pr√©dit plus les valeurs, mais la probabilit√© $p(y = 1|x)$ compos√©e avec la fonction logistique $u\mapsto {1\over{1+e^{-u}}} $
    - Pas de solution exacte, calcul num√©rique par gradient
    - Pour √©viter le sur-apprentissage, r√©gularisation  ‚Ñì2 (par d√©faut dans `scikit-learn`, 
    - Pour un mod√®le parcimonieux, r√©gularisation ‚Ñì1 (dans `scikit-learn`, option`'penalty'=l1`
- SVM binaire = support vector machine = separatrice a vaste marge
    - recherche d'un hyperplan s√©parateur maximisant la marge
    - risque d'erreur (observations impossibles √† s√©parer par hyperplan, typiquement outliers). On utilise Hinge loss = perte charniere
    - 
- SVM multiclasse : regression multiple classes
    - one-versus-rest OVR = One-versus-all = OVA
        - on construit k SVM, en cherchant √† optimiser
    - one-versus-one OVO
- evaluer la qualit√© d'une pr√©diction
        - [`sklearn.metrics.mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html') pour calculer MSE ou RMSE entre la pr√©diction et la r√©alit√©. (R= root square)\
</details>

<details>
<summary> <h3> M.3 Mod√®les pr√©dictifs non lin√©aires </h3> </summary>
- "Kernel trick" : transformer les x d'input des 
![image](https://user-images.githubusercontent.com/7408762/197527731-29e2ad2b-2a1e-48a7-b92c-26df55445280.png)

- Neural networks : fonction d'activation sur entr√©es. "Perceptron"
    - Le Perceptron = "neurone" : 
        - Combi lin√©aire des entr√©es x activation
        - poids appris par descende de gradient
    - Empiler les perceptrons : 
        - poids sur chaque perceptron
        - √† entrainer avec EN back-propagation (FR r√©tro-propagation) : $derreur/dw_hji= d/d * d/d * d/d$

    - Pour approximation
        - technique de descente du gradient
        - entropie crois√©e 
    - Pour classification
        - possible d'utiliser activation √† seuil
        - mieux : utiliser sigmoide (typiquement : activation logistique) pour probabilit√© d'appartenance √† une classe 
    - Limitation : les r√©seaux de neurones ne sont pas la solution √† tous les probl√®mes car...
        
</details>


<details>
<summary> <h3> M.4 Mod√®les ensemblistes </h3> </summary>

- Gist = combine several models together
    - "Bootstrap" first idea = sampling with remise √©chantillonage avec remise
    - M√©thodes parall√®les: train several models simultaneously, recombine them at the end
    - utilisant des "apprenants faibles" : des m√©thodes simples et peu efficaces, qui en se combinant donnent de meilleurs r√©sultat que les m√©thodes complexes
    - M√©thodes s√©quentielles : **boosting**

<details>
<summary>
- **Bagging** = Bootstrap aggregation </summary>
    - Moyenne pour pr√©diction, vote majoritaire pour classification
    - 
```(python)
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=100, noise=0.25)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

from sklearn.ensemble import BaggingClassifier 

bagging = BaggingClassifier(n_estimators=5)
bagging.fit(X_train, y_train)
from mglearn.plot_interactive_tree import plot_tree_partition
from mglearn.plot_2d_separator import plot_2d_separator
from mglearn.tools import discrete_scatter

fig, axes = plt.subplots(2, 3, figsize=(20, 10))
for i, (ax, tree) in enumerate(zip(axes.ravel(), bagging.estimators_)):
    ax.set_title("Tree {}".format(i))
    plot_tree_partition(X_train, y_train, tree, ax=ax)
plot_2d_separator(bagging, X_train, fill=True, ax=axes[-1, -1],
                                    alpha=.4)
axes[-1, -1].set_title("Bagging")
discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)
```

</details>

<details>
<summary>
- **Random Forest** = arbres de d√©cisions binaires combin√©s  la majorit√© de vote
</summary>
    - Pb: les arbres de d√©cision ont tendance √† overfitter. 
    - Pour faire grandir chaque noeud, on n'utilise qu'un sous-ensemble de features (et pas toutes comme le bagging).
        - sous ensemble choisi de mani√®re al√©atoire : arbres al√©atoires
    - Avantage  : complexit√© peu √©lev√©s, on a estimation de l'importance des features. Pas d'overfitting, peu de m√©moire utilis√©e. 
 ```(python)   
import pandas as pd

train = pd.read_csv("train.csv")
test  = pd.read_csv("test.csv")
print(train.shape)
train.isna().sum()
train = train.loc[train.Activity.notna()]
train = train.fillna(train.median(), inplace=True)

X_train = train[train.columns[:-2]]
y_train = train['Activity']

X_test = test[test.columns[:-2]]
y_test = test['Activity']

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=500, oob_score=True)
model = rfc.fit(X_train, y_train)
from sklearn.metrics import accuracy_score

pred = rfc.predict(X_test)
print("accuracy {:.2f}".format(accuracy_score(y_test, pred)))
from sklearn.feature_selection import SelectFromModel
select = SelectFromModel(rfc, prefit=True, threshold=0.003)
X_train2 = select.transform(X_train)
print(X_train2.shape)
import timeit

rfc2 = RandomForestClassifier(n_estimators=500, oob_score=True)

start_time = timeit.default_timer()

rfc2 = rfc2.fit(X_train2, y_train)

X_test2 = select.transform(X_test)

pred = rfc2.predict(X_test2)
elapsed = timeit.default_timer() - start_time
accuracy = accuracy_score(y_test, pred)

print("accuracy {:.2f} time {:.2f}s".format(accuracy, elapsed))

```

</details>

<details>
<summary> Boosting & Gradient Boosting </summary>

- Le Boosting, dont adaboost
    - on pondere chacun des points √† chaque generation

- Gradient de Boosting : 
    - jhkjhj

</details>

</details>

</details>

<details>
<summary> Sources </summary>
---
[Cheatsheet Anthony : https://asardell.github.io/statistique-python/](https://asardell.github.io/statistique-python/)

[Meme contenu copi√© sur evernote](evernote:///view/6367254/s57/f1dae14f-b0c0-4024-a6f5-7b2535f53308/67117fc9-036c-4028-b61e-04a2b3349d73/)
</details
>
---

***


chgt 02nov22 2021

*** 
