# ğŸ“ŒCheatsheet | Statistics

---
[Cheatsheet Anthony : https://asardell.github.io/statistique-python/](https://asardell.github.io/statistique-python/)

[Meme contenu copiÃ© sur evernote](evernote:///view/6367254/s57/f1dae14f-b0c0-4024-a6f5-7b2535f53308/67117fc9-036c-4028-b61e-04a2b3349d73/)

---

<details>
<summary> <h2>Vocabulaire </h2></summary>

*   Stat descriptives (qui mesure) vs probabilitÃ©s = statistiques infÃ©rentielles (qui prÃ©dit) : _descriptive sur le passÃ©, infÃ©rentielle sur le futur)_ 
    

*   En stat infÃ©rentiel, on utilise des tests satistiques = estimateurs, on crÃ©e des modÃ¨les statistiques
    

*   lignes = individus = unitÃ© d'observation = rÃ©alisation
    
*   colonnes = variables = caractÃ¨res

* population vs Ã©chantillon = jeu de donnÃ©es = dataset = observation
    
*   variables quantitatives : 
    * discrÃ¨tes vs continues
    
*   variables qualitatives => modalitÃ©s
    *   soit nominale, soit ordinale    

* Noir = nominal (quali), ordinal (#quali), interval, ratio (all quanti)

    
*   ordinale ( e.g. dates, timestamp). [Conversion](www.epochconverter.com)
    
*   boolÃƒÂ©enne
    
</details>


<details>

<summary> <h2>Nettoyage </h2></summary>

*   PrÃ©voir Aller retour nettoyage et analyse
    
*   Valeurs manquantes 

```myDF.isnull().sum() #somme par colonne le nb de manquant```Â 
    

1.  Trouver la bonne valeur (ÃƒÂ  la main)
    
2.  Travailler avec un gruyÃƒÂ¨re
    
3.  Oublier la variable
    
4.  Oublier les individus (mais les individus restants ne sont pas forcÃƒÂ©ment reprÃƒÂ©sentatifs)
    
5.  Imputer (= deviner, e.g. imputation par la moyenne, ou imputer intelligemment, eg selon ÃƒÂ¢ge pour la taille)
    

*   Traiter les outliers (= valeur aberrantes)
    
    trouvÃƒÂ©es par Z-score ou ÃƒÂ©cart interquartile
    

1.  Trouver la bonne valeur (ÃƒÂ  la main)
    
2.  Supprimer la valeur ou conserver la valeur ... en fonction des ÃƒÂ©tudes (e.g. moyenne vs mÃƒÂ©diane)
    
3.  ... les valeurs atypiques sont intÃƒÂ©ressantes, et ÃƒÂ  mentionner
    

*   Eliminer les doublons... si on peut
    

*   Regrouper en gÃ©rant les contradictions
    

*     
    

  

MÃƒÂ©thodeÃ‚Â :

*   MÃƒÂ©thodeÃ‚Â :
    

*   AllerÃ‚Â retour nettoyage et analyse
    
*   ValeursÃ‚Â manquantes :
    

*   TrouverÃ‚Â la bonne valeur (ÃƒÂ  la main)
    
*   TravaillerÃ‚Â avec un gruyÃƒÂ¨re
    
*   OublierÃ‚Â la variable
    
*   OublierÃ‚Â les individus (mais les individus restants ne sont pas forcÃƒÂ©ment reprÃƒÂ©sentatifs)
    
*   ImputerÃ‚Â (= deviner, e.g. imputation par la moyenne, ou imputer intelligemment, eg selon ÃƒÂ¢ge pour la taille)
    

*   TraiterÃ‚Â les outliers (= valeur aberrantes)
    
*   trouvÃƒÂ©esÃ‚Â par Z-score ou ÃƒÂ©cart interquartile
    

*   TrouverÃ‚Â la bonne valeur (ÃƒÂ  la main)
    
*   SupprimerÃ‚Â la valeur ou conserver la valeur ... en fonction des ÃƒÂ©tudes (e.g. moyenne vs mÃƒÂ©diane)
    
*   ...Ã‚Â les valeurs atypiques sont intÃƒÂ©ressantes, et ÃƒÂ  mentionner
    

*   EliminerÃ‚Â les doublons... si on peut
    

*   RegrouperÃ‚Â en gÃƒÂ©rant les contradictions
    

MÃƒÂ©thodeÃ‚Â :

*   AllerÃ‚Â retour nettoyage et analyse
    
*   ValeursÃ‚Â manquantes :
    

*   TrouverÃ‚Â la bonne valeur (ÃƒÂ  la main)
    
*   TravaillerÃ‚Â avec un gruyÃƒÂ¨re
    
*   OublierÃ‚Â la variable
    
*   OublierÃ‚Â les individus (mais les individus restants ne sont pas forcÃƒÂ©ment reprÃƒÂ©sentatifs)
    
*   ImputerÃ‚Â (= deviner, e.g. imputation par la moyenne, ou imputer intelligemment, eg selon ÃƒÂ¢ge pour la taille)
    

*   TraiterÃ‚Â les outliers (= valeur aberrantes)
    
*   trouvÃƒÂ©esÃ‚Â par Z-score ou ÃƒÂ©cart interquartile
    

*   TrouverÃ‚Â la bonne valeur (ÃƒÂ  la main)
    
*   SupprimerÃ‚Â la valeur ou conserver la valeur ... en fonction des ÃƒÂ©tudes (e.g. moyenne vs mÃƒÂ©diane)
    
*   ...Ã‚Â les valeurs atypiques sont intÃƒÂ©ressantes, et ÃƒÂ  mentionner
    

*   EliminerÃ‚Â les doublons... si on peut
    

*   RegrouperÃ‚Â en gÃƒÂ©rant les contradictions
    
</details>

<details>
<summary> <h2>Erreurs et imputations </h2></summary>

7 types d'erreurs :
1.  Valeurs manquantes
2.  **Erreur lexicale** (e.g. texte quand nombre attendu, ou liste limitative de pays possibles,,)
3.  **IrrÃ©gularitÃ©** (e.g. cm quand m attendu)
    
4.  formatage incorrect
5. formatage parfois liÃ© Ã  hypothÃ¨ses de contenu ( e.g. 2 emails pour 1 personne))
6.  **doublon** (+ parfois **contradiction** si les doublons ont des valeurs diffÃ©rentes)
7.  valeur extrÃ¨me = **atypique** (pas fausse) ou **aberrante** (fausse)
    
Comment rÃ©soudre les erreurs 

0. On peut **suprrimer** les individus avec erreur ... si ceux qui restent sont suffisants / non biaisÃ©s.

1. Valeur manquante = 
    * **imputation** e.g. imputation par la moyenne (simple) -> mÃ©thode de hot-deck, Machine Learning / KNN, rÃ©gressions
    * ou travailler avec un gruyÃ¨re (donnÃ©es Ã  trou, selon le traitement statistique)

6.  Doublon 
    * methodes `myDF.duplicated() myDF.duplicate() myDF.unique()`
    * contradiction : Ã  ignorer, ou prendre la moyenne
    * parfois regroupement (information 1 individu rÃ©partie sur plusieurs lignes)

7.  Valeur extrÃ¨meÂ  = 
    * choix des traitements, e.g. la moyenne est sensible aux outliers, pas la mÃ©diane 
    * midspread ou Z-score, 
    * boite a moustache (boxplot)

</details>    

* * *
<details>
<summary> <h2> ReprÃƒÂ©senter des variables </h2> </summary>

  

  

  

  

  

  

  
</details>
* * *

<details>
<summary> <h2> ReprÃƒÂ©senter des variables </h2> </summary>

SupervisÃƒÂ© => j'ai dÃƒÂ©ja des tag d'apprentissage. On parle de **classement**\= classification supervisÃƒÂ©e (en EN = "classification").Ã‚Â 

Non supervisÃƒÂ© =Ã‚Â  **clusteringÃ‚**Â 

![](Ã°Å¸â€œÅ’Cheatsheet  Statistics_files/Image.png)

  

D

* * *

Distance (erreur = risque = eloignement des donnÃƒÂ©es vs prediction modele)

Attention : erreur = risque empirique != performance du modele

*   erreur quadratique (le + utilisÃƒÂ©)
    

*   distance euclidienne = sqr(x^2 + y^2)
    

*   Distance manhattan = x + y
    
*   Pour chaines de caracteres = distance de Levenshtein = nbre mini d'operation (substitution, insertion, suppression) pour passer de l'une a l'autre.Ã‚Â 
    

*   a connaitre = algo de Wagner et Fischer pour le calcul de la distance de Levenshtein.
    

algo paramÃƒÂ©triques (eg regression = droite) => on cherche le parametreÃ‚Â ÃÂ¸ (qui peut etre multidimensionel)

algos non parametriques (+ complexitÃƒÂ©) => egg k-means qui est 'memory based' (garde toutes les donnÃƒÂ©es en memoire)

  

  

  

fuction loss = perte d'information

vraisemblance d'un jeu d'observations (x1...xN) par rapport ÃƒÂ  un modÃƒÂ¨le en statistiques est la fonction suivante :Ã‚Â Ã‚Â L(ÃÂ¸)=p(x1...xN|ÃÂ¸)Ã‚Â Ã‚Â .= proba d'avoir x1...xN sachant \\Theta

Ã‚Â ÃÂ¸^Ã‚Â avec un accent circonflexeÃ‚Â lorsqu'on parle d'unÃ‚Â estimateur (eet non de la valeur reelle, intrinseque)

  

* * *

1.  MÃƒÂ©thode factorielle = la + connue ACP
    
2.  Clulstering = Classification non supervisÃƒÂ©e = la + connue k-means (K-moyennes)
    

  

Factorielle :Ã‚Â 

ACPÃ‚Â  ( = EN PCA) = Principal component analysis

*   Ã‚Â  Ã‚Â  recehche d'un (hyperplan) avec moment d'inertie max (ÃƒÂ©talement des points) = axe orthogonal ÃƒÂ  l'hyperplan = donne indication sur la variabilitÃƒÂ© =
    

*   espace Rp de dimension p variables, contient Ni le nuage des individus
    

*   Rechreche des corrÃƒÂ©lations entre variablesÃ‚Â 
    

*   espace Rn de dimension n individus, contient Np le nuage des variables
    

De prÃƒÂ©fÃƒÂ©rence ACP normÃƒÂ©e (centrÃƒÂ©e rÃƒÂ©duite)

3 graphiques :Ã‚Â 

1.  1\. Pour l'objectif 1, ce sera la projection du nuage des individus NI sur les 2 premiers axes dÃ¢â‚¬â„¢inertie, cÃ¢â‚¬â„¢est-ÃƒÂ -dire sur le premier plan factoriel.
    
2.  Le second sÃ¢â‚¬â„¢appelle le cercle des corrÃƒÂ©lations.
    
3.  2\. Pour l'objectif 2, ce sera la projection du nuage des variables NK sur le premier plan factoriel.
    

  

combien de composantes = min (p nbr de varialbes et n-1 nombre individus)

\=> eboulis des valeurs propres (classÃƒÂ©es en valeur dÃƒÂ©croissante)

\=> frequent de n'analyser que le 1er plan (2 composantes). Critere du coude - reperer le # oÃƒÂ¹ le % inertie diminue + lentement. Criter de Kaiser (~contribution moyenen 100% / p)

  

k-meansÃ‚Â 

k est un **hyperparamÃƒÂ¨tre** (c'est ÃƒÂ  nous de l'optimiser, ce n'est pas l'algo qui va le proposer).Ã‚Â 

  

  

Trainig set vs testing set = 80% / 20% des donnÃƒÂ©es fournies

  

  

* * *

Conversion de timestamp unix =Ã‚Â Ã‚Â [www.epochconverter.com](http://www.epochconverter.com/)Ã‚Â !

  

Erreur lexicale => Technique du dictionnaire.

Date => Format normalisÃƒÂ© ISO8601Ã‚Â 1977-04-22T06:00:00Z.

  

  

</details> 

* * *

<details>  
<summary>
**Comment centrer reduire :** </summary>

import pandas as pd

import numpy as np

from sklearn.preprocessing import StandardScaler

DÃƒÂ©finissons nos donnÃƒÂ©es :

  

\# Notre matrice de base :

X = \[\[12,Ã‚Â  Ã‚Â  30,Ã‚Â  Ã‚Â  80,Ã‚Â  -100\],Ã‚Â  Ã‚Â  \[-1000, 12,Ã‚Â  Ã‚Â  -23,Ã‚Â  10\],Ã‚Â  Ã‚Â  \[14,Ã‚Â  Ã‚Â  1000,Ã‚Â  0,Ã‚Â  Ã‚Â  0\]\]

  

\# Version numpy :

X = np.asarray(X)

\# Version pandas :

X = pd.DataFrame(X)

Avec Ã‚Â pandasÃ‚Â  , on peut calculer la moyenne et l'ÃƒÂ©cart-type de chaque dimensionÃ‚Â :

  

\# On applique la methode .describe() pour avoir la moyenne et la .std(), et la mÃƒÂ©thode .round(2) pour arrondir ÃƒÂ  2 dÃƒÂ©cimales aprÃƒÂ¨s la virgule :

X.describe()

On peut ensuite Ã‚Â«Ã‚Â scalerÃ‚Â Ã‚Â» nos donnÃƒÂ©es :

  

\# On instancie notre scaler :

scaler = StandardScaler()

\# On le fit :

scaler.fit(X)

\# On l'entraine :

X\_scaled = scaler.transform(X)

\# On peut faire les 2 opÃƒÂ©rations en une ligne :

X\_scaled = scaler.fit\_transform(X)

\# On le transforme en DataFrame :

X\_scaled = pd.DataFrame(X\_scaled)

\# On peut appliquer la mÃƒÂ©thode .describe() et .round()

X\_scaled.describe().round(2)

</details>

***

<!--details-->  


<summary>
**Modeles prÃ©dictifs:** </summary>
  
- modeles predictifs linÃ©aires
    - Si linÃ©aritÃ©+normalitÃ©+indÃ©pendance (i.i.d.) 
        - => regression, recherche $Î²$ qui maximise la vraissemblance ($p(D|Î²)$) = minimise la somme des carrÃ©s des erreurs
        -  `LinearRegression` dans le module `linear_model`.
        - $Î²=(X^âŠ¤X)^{âˆ’1}X^âŠ¤y$ 
        - ... et si $X^TX$ non inversible, utiliser pseudo-inversible
    - si correlation, ou trop peu d'observation, la matrice des XtX n'est pas inversible => Sur-apprentissage car modele trop complexe
        - => Alors on minimise une fonction objectif = erreur + complexitÃ© - minimum en $Î²$ du carrÃ© des erreurs + Î».rÃ©gularisateur(Î²) 
        - oÃ¹ $Î»$ hyperparamÃ¨tre du poids de la regularisation (cf validation croisÃ©e)
        - **rÃ©gularisation de Tykhonov = ridge regression** oÃ¹ regulateurs=carrÃ© de la norme de $Î²$ 
            - dans `scikit-learn : linear_model.Ridge` et `linear_model.RidgeCV` pour dÃ©terminer la valeur optimale du $Î»$ par validation croisÃ©e.
            - => toujours solution unique explicite $Î²=(Î»I+X^âŠ¤X)^{âˆ’1}X^âŠ¤y$
            - mais il faut toujours standardiser les variables $X$ pour $Ïƒ=1$ avec `sklearn.preprocessing.StandardScaler`
            - chemin de rÃ©gression : comment Ã©voluent les $Î²_j$ avec $Î»$
[image](cheminregression.png)
        - **Lasso = modele parcimonieux (_sparse_)** pour rÃ©duire nombre de coeff $Î²$ = en avoir bcp nuls = 0
            - on utilise regularisateur norme1 de $Î²$
            - lasso = _Least Absolute Shrinkage and Selection Operator_
            - si plusieurs variables corrÃ©lÃ©es, le Lasso va en choisir une seule au hazard => modele instable, solution non unique
            - Lasso est un algo de rÃ©duction de dimension non supervisÃ©
        - **selection groupÃ©e = elastic net** 
            - consiste Ã  combiner normes 1 et 2 sur beta, avec cette fois 2 hyperparamÃ¨tres 
            - => solution moins parcimonnieuse, mais plus stable que Lasso
    - regression logistique = pour classification binaire
        - classification binaire
        - SVM = support vector machine = separatrice a vaste marge
        - Hinge loss = perte charniere
        - 
    - regression multple classes
        - one-versus-rest OVR = One-versus-all = OVA



  

</details-->

* * *
